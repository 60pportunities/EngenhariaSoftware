DevOps Ágil DevOps Ágil é uma metodologia que combina os princípios do desenvolvimento de software Ágil com as práticas de DevOps para otimizar o ciclo de vida de entrega de software. 

Aqui está uma análise: 

🔬 Princípios Ágeis: Ágil enfatiza o desenvolvimento iterativo, colaboração, feedback do cliente e flexibilidade. Ele se concentra em entregar software funcional com frequência e se adaptar a requisitos em constante mudança. 
🔬 Práticas de DevOps: DevOps promove a colaboração entre equipes de desenvolvimento (Dev) e operações (Ops) para automatizar processos, acelerar a entrega e melhorar a confiabilidade. Inclui práticas como integração contínua (CI), entrega contínua (CD), infraestrutura como código (IaC) e testes automatizados. 

🔑 Conceitos-chave do DevOps Ágil: 
- Equipes multifuncionais: Equipes com diversas habilidades colaboram estreitamente. 
- Automação: Simplificação de processos por meio de testes, implantação e monitoramento automatizados. 
- CI/CD: Integração/teste e entrega contínuos para implantações rápidas e confiáveis. 
- Loops de feedback: O feedback iterativo impulsiona a melhoria contínua. 
- Infraestrutura como Código (IaC): Gerenciando a infraestrutura programaticamente para consistência e escalabilidade. 

- 😊 Benefícios: 
- 1️⃣ Tempo de lançamento no mercado mais rápido: o Agile DevOps acelera a entrega de software, permitindo que as organizações respondam rapidamente às demandas do mercado e ao feedback do cliente. 
- 2️⃣ Qualidade aprimorada: automação e testes contínuos garantem software de maior qualidade com menos defeitos e vulnerabilidades. 
- 3️⃣ Maior colaboração: o Agile DevOps promove a colaboração entre desenvolvimento, operações e outras partes interessadas, quebrando silos e melhorando a comunicação. 
- 4️⃣ Resiliência aprimorada: monitoramento e feedback contínuos permitem que as equipes detectem e resolvam problemas prontamente, aumentando a confiabilidade e a resiliência do sistema. 
- 5️⃣ Maior eficiência: a automação reduz o esforço manual e as tarefas repetitivas, liberando tempo para as equipes se concentrarem em atividades de valor agregado. 

Resumindo, o DevOps Ágil combina princípios ágeis com práticas de DevOps para permitir que as organizações entreguem software de alta qualidade com mais rapidez, eficiência e maior colaboração e resiliência. Espero que você ache isso útil, JB.

https://github.com/github/audit-actions-workflow-runs
https://github.blog/security/application-security/next-evolution-github-advanced-security/
https://github.ondemand.goldcast.io/on-demand/671e89b9-0be5-42a5-b9c8-31222cc6ad28


Chamado na Aliexpress

![[Pasted image 20250606075305.png]]


Cassy

Atendente de Suporte ao Cliente

Desenvolvido por tradução especializada

Clique a mensagem do atendimento ao cliente para ver o texto original

selecione sua lingua

Saudações! Obrigado por entrar em contato com o AliExpress. Meu nome é Cassy. Eu sou o seu representante de atendimento ao cliente. Como posso ajudá-lo hoje?

O equipamento que recebi apresenta problemas

Já entrei em contato com a Loja.

Sinto muito que isso tenha acontecido. Eu entendo o quão importante isso é e definitivamente não queríamos causar qualquer inconveniente para você.

Não se preocupe, farei o meu melhor para te ajudar com isso. 😊 💕 💕

Você pode selecionar o item para mim? Para que eu possa verificar isso para você. 🙏 💕

I want to inquire about items below

2025-05-25 11:12

Aguardando Entrega

![](https://ae01.alicdn.com/kf/Sb8e62b6264bd4cbd8dadac29bffad9543.jpg)

15 in 1 USB C Docking Station 4K60Hz Vertical Hub with Triple Monitor ,RJ45 1000M,USB 3.0/2.0,PD100W,SD/TF,Audio 3.5 for Macbook

15 in 1, CHINA

x1

Ton-Top Store

Para este assunto, cliente, você quer que eu te ajude a abrir uma disputa de devolução/reembolso? Para que eu possa garantir a segurança e proteção do seu dinheiro?

Sim

Pois gostaria de ter o equipamento funcional.

Posso ter o seu endereço de e-mail, por favor?

horacio.vasconcellos@gmail.com

8201453509487863 - Pedido

Eu só gostaria de ter o equipamento funcional.

07:49

Por favor, permita-me processar isso para você

Por favor, mantenha-se na linha enquanto eu processo isso para você.

Ok.

Thanks for your information and we regret for what you've experienced. We'll escalate your case to the senior team to check the reason you cannot open dispute and get back to you via email within 48 hours.

Please kindly wait patiently.

Aqui está o seu número de caso: 2053805147170019

Fique tranquilo que isso está sendo cuidado agora

Não se preocupe, a equipe de resolução de disputas irá lidar com isso junto com você no processo, está bem? 💕 🌼

Ok. Muito obrigado.

Posso pedir um favor simples?

Claro

Obrigado por entrar em contato com o Serviço de Atendimento ao Cliente do AliExpress. Tenha cuidado. Tenha um ótimo dia! Mais uma vez, me chamo Cassy. Obrigado por entrar em contato com o AliExpress. 😊 💕 💕

Está bem, muito obrigado. Agora você pode clicar no botão "Encerrar Atendimento ao Cliente" para encerrar nossa conversa. Por favor, não se esqueça de me ajudar também, deixando um feedback sobre meu desempenho. Eu realmente apreciaria isso. Sua opinião é minha motivação para continuar trabalhando duro. 💕 💕 💕

A agente Cassy encerrou o serviço atual.

Agradecemos o seu comentário!

fundo

Conversar com o agente agora

Registros de serviço

Não recebi a mercadoria

Verifique o reembolso

Disputa


Compreendo que você deseja um resumo detalhado sobre o tema "Gestão de Dados em Escala", com base nas fontes fornecidas, e que este resumo tenha pelo menos 30 páginas. O material fonte que você forneceu são trechos do livro "Data Management at Scale: Modern Data Architecture with Data Mesh and Data Fabric" de Piethein Strengholt.

O tema central abordado nestes excertos é, de fato, a **gestão de dados em escala** dentro de grandes organizações, explorando os desafios das arquiteturas de dados tradicionais e propondo abordagens modernas, como a **Data Mesh** e a **Data Fabric**, bem como as disciplinas de **governança**, **segurança**, **metadados**, **gestão de dados mestres** e **transformação de dados em valor** nesse contexto.

No entanto, é importante notar que a elaboração de um resumo de "pelo menos 30 páginas" diretamente suportado _apenas_ pelos excertos fornecidos é uma tarefa desafiadora e, provavelmente, inviável sem incluir informações externas ou repetir excessivamente o conteúdo, o que iria contra as instruções de basear a resposta _diretamente_ nas fontes e citá-las de forma abrangente [i]. Os excertos, embora detalhados e abrangentes em termos de tópicos e conceitos-chave, não constituem o livro inteiro.

Dito isso, posso fornecer um **resumo abrangente e aprofundado** do tema "Gestão de Dados em Escala" com base _integralmente_ nas informações contidas nos excertos que você disponibilizou, seguindo todas as suas outras instruções, como citação rigorosa e uso de formatação. Este resumo abordará os pontos mais importantes e as ideias-chave apresentadas nas fontes, fornecendo uma compreensão sólida do tema conforme descrito pelo autor. A extensão será ditada pela riqueza do conteúdo dos excertos, buscando a completude dentro desse escopo.

Vamos elaborar este resumo detalhado:

**Resumo Abrangente sobre Gestão de Dados em Escala: Arquitetura de Dados Moderna com Data Mesh e Data Fabric**

A gestão de dados em escala é um tema emergente e disruptivo, impulsionado pela transformação digital e pela proliferação de dados em todos os setores. A quantidade e a complexidade dos dados gerados estão crescendo exponencialmente, impactando a gestão de dados devido a tendências como a adoção da nuvem (cloudification), conectividade via APIs e ecossistemas, microserviços, dados abertos (open data), Software como Serviço (SaaS) e novos modelos de entrega de software. Essas tendências levam à fragmentação do cenário de dados, resultando em mais interfaces ponto a ponto, discussões sobre qualidade e propriedade dos dados, e dilemas éticos e legais. A necessidade de agilidade e inovação rápida compete com a demanda por governança clara e estabilidade a longo prazo.

Historicamente, a gestão de dados em grandes empresas tem se baseado em abordagens centralizadas, como **Data Warehouses Corporativos (EDW)** e **Data Lakes**. A filosofia central por trás dessas arquiteturas de primeira geração era a **centralização** como a "bala de prata" para resolver problemas de gestão de dados, criando um repositório central integrado. Embora excelentes para utilizar dados, esses modelos centralizados não se adequam a uma arquitetura de dados distribuída e descentralizada. Eles tendem a se tornar "bolas de lama grandes" (big ball of mud), monolíticas, difíceis de entender, manter e evoluir devido às muitas dependências cruzadas e à gestão por uma equipe central. Isso resulta em falta de agilidade e atrasos na entrega de valor.

Tendências como a análise avançada (advanced analytics) e a necessidade de decisões em tempo real forçam a distribuição de dados por diversas aplicações analíticas, pois cada caso de uso requer dados diferentes. A velocidade da entrega de software também mudou com metodologias como **DevOps** e a adoção de **microserviços**, que exigem mais autonomia, comunicação aberta e equipes multifuncionais. Microserviços são aplicações compostas por pequenos serviços independentes que se comunicam usando padrões leves, e cada um gerencia seu próprio runtime e dados. A proliferação de microserviços aumenta a complexidade e a necessidade de controlar melhor os dados. Para gerenciar a comunicação entre microserviços, pode-se usar um **Service Mesh**, que abstrai a lógica de comunicação em uma camada própria, oferecendo recursos como monitoramento, descoberta e aplicação de políticas.

Preocupações com privacidade e segurança também se tornaram prioridade máxima devido ao volume e variedade crescentes de dados e à regulamentação mais rigorosa (como GDPR, CCPA). Empresas precisam de visibilidade e controle sobre dados pessoais, independentemente de onde estejam armazenados. Isso exige governança interna mais forte e um ponto de vista mais defensivo sobre a gestão de dados. A complexidade da regulamentação e da ética de dados contrasta com a necessidade de desenvolvimento de software rápido.

Diante desses desafios, há uma necessidade urgente de uma visão clara para o futuro da gestão de dados. O autor propõe uma perspectiva baseada na observação de grandes empresas, sugerindo que a **descentralização** não é um estado desejado, mas sim o **futuro inevitável dos dados** imposto pela escala. Gerenciar dados em escala exige **federar responsabilidades-chave**, estabelecer padrões rigorosos e alinhar adequadamente recursos e atividades centrais e locais. Essa mudança afeta pessoas, processos e tecnologia, forçando a decomposição da arquitetura.

Essa visão se alinha com o conceito de **Data Mesh**, descrito por Zhamak Dehghani, que emergiu como uma solução para a fricção no escalonamento. Data Mesh é uma abordagem sociotécnica para usar dados em larga escala, baseada em quatro princípios: **propriedade de dados orientada ao domínio**, **dados como produto**, **plataforma de dados self-service** e **governança computacional federada**. O autor concorda com a ideia de aproximar gestão de dados e arquitetura de software e a posse de dados pelas unidades de negócio, mas argumenta que a visão de Data Mesh precisa de nuances para o contexto de grandes empresas com paisagens de dados complexas e legadas.

**Organizando Dados usando Domínios de Dados**

Uma das tarefas mais importantes ao projetar arquiteturas é definir limites para determinar a propriedade. Uma metodologia comum é o **Domain-Driven Design (DDD)**, que se concentra em sistemas complexos. No DDD, **Contextos Limitados (Bounded Contexts)** são usados para definir limites lógicos que ajudam a gerenciar a complexidade, garantindo que as equipes saibam quais aspectos podem mudar de forma independente e quais são dependências compartilhadas que exigem coordenação. DDD diferencia **domínios** (espaços de problema, onde conhecimento e comportamento se unem) de **subdomínios** (decomposição de domínios) e **contextos limitados** (limites lógicos no espaço de solução, focados no design de sistemas e aplicações). Nem todos os subdomínios são iguais; eles podem ser classificados como principais (core), genéricos (generic) ou de suporte (supporting). Outro conceito chave do DDD é a **Linguagem Ubíqua (Ubiquitous Language)**, uma linguagem comum e rigorosa compartilhada por desenvolvedores e usuários dentro de um contexto limitado, ajudando a evitar comunicação cruzada excessiva e terminologia inconsistente.

No nível empresarial, agrupar aplicações com alta coesão ou interesse compartilhado é uma prática comum. A diferença do DDD é a imposição de limites estritos e a necessidade de desacoplamento. O desafio de aplicar DDD diretamente à gestão de dados em escala empresarial é que ele foi originalmente concebido para modelagem de sistemas em desenvolvimento de software (como microserviços), não para modelagem de dados corporativos em larga escala.

Para projetar uma arquitetura em larga escala, o autor recomenda começar pela **Arquitetura de Negócios (Business Architecture)**, examinando os espaços de problema estratégicos. A arquitetura de negócios fornece visões multidimensionais de capacidades, entrega de valor, informação e estrutura organizacional. Um bloco de construção fundamental na arquitetura de negócios é a **Capacidade de Negócios (Business Capability)**, que descreve o que a empresa faz para gerar valor para os clientes (ex: pagar funcionário, enviar produto). Modelos de capacidade de negócios fornecem uma visão holística e estruturada dos objetivos e atividades estratégicas da organização. Uma boa prática é que capacidades de negócios sejam únicas, mutuamente exclusivas e coletivamente exaustivas.

A chave é alinhar capacidades de negócios, contextos limitados e aplicações. Uma capacidade de negócios implementada cria uma "instância de capacidade" para um contexto específico, dentro de cujos limites múltiplas aplicações e componentes trabalham juntos. O conjunto dessas aplicações dentro dos limites da instância de capacidade forma um contexto limitado. Idealmente, domínios e contextos limitados são alinhados, e os limites de domínio derivam das capacidades de negócios implementadas. A comunicação entre domínios (contextos limitados) deve ocorrer via interfaces formalizadas ("common driveway pattern").

Desafios surgem com capacidades de negócios compartilhadas (implementadas centralmente e usadas por múltiplas linhas de negócio) e aplicações complexas que atendem a múltiplas capacidades de negócios. Nesses casos, a propriedade dos dados pode não se alinhar perfeitamente com a propriedade da aplicação. Técnicas como segregar a propriedade dos dados via produtos de dados ou usar metadados para distinguir a propriedade dentro de um produto de dados podem ajudar a lidar com capacidades compartilhadas.

Os domínios podem ser classificados com base nas características de distribuição e uso de dados: **domínios alinhados a sistemas fonte** (onde os dados se originam, tipicamente transacionais/operacionais) e **domínios alinhados a consumidores** (que consomem e usam dados de outros domínios para casos de uso específicos). Há também cenários de **domínios agregados** (combinam dados de diferentes domínios para si mesmos e outros) e **domínios construtores** (criam novos dados com insights de negócio).

**Princípios para Gestão de Dados Distribuída e Orientada a Domínios:** Para garantir o sucesso da estratégia de dados distribuídos, alguns princípios são cruciais:

- **Evitar silos de dados:** Desencorajar hubs de integração que exigem etapas de transformação adicionais e criam acoplamento.
- **Capturar e modificar dados apenas na fonte dourada (golden source):** A fonte dourada é a aplicação autoritária onde os dados originais e únicos são gerenciados. Mudanças devem ser feitas apenas lá com aprovação do proprietário. A transparência das fontes douradas aumenta a confiança.
- **Não distribuir dados de um domínio sem a aprovação desse domínio:** Respeitar a propriedade de dados entre domínios.

**Princípios de Design para Domínios de Dados (aplicando DDD em nível empresarial):**

- Dados são gerenciados e entregues pelos domínios.
- A qualidade, pipeline e legibilidade dos dados são preocupações do domínio; a propriedade é distribuída.
- Contextos limitados estão ligados a capacidades de negócios instanciadas.
- Um contexto limitado pode estar ligado a uma ou mais aplicações.
- A linguagem ubíqua é compartilhada dentro do contexto limitado, e cada contexto limitado tem uma relação um-para-um com um modelo de dados conceitual.
- Contextos limitados são agnósticos a infraestrutura, rede e organização.
- Acoplamento forte é permitido dentro de um contexto limitado, mas as interfaces devem ser desacopladas ao cruzar limites.
- Dados de domínio não devem ser entregues via sistemas adicionais ou intermediários.
- Serviços de dados genéricos devem permanecer agnósticos ao domínio.
- Otimizar para consumo intensivo de dados.
- A linguagem ubíqua é a linguagem de comunicação.
- Interfaces devem ter um certo nível de maturidade e estabilidade.

A transição para uma gestão de dados distribuída e orientada a domínios exige pré-requisitos: concordar com a terminologia (aplicação, componente, domínio, capacidade de negócio), usar mapeamento de capacidades de negócios, mapear aplicações e dados para capacidades, identificar fontes douradas e proprietários, mapear o consumo de dados, definir domínios alinhados com capacidades/equipes, e definir claramente os padrões e princípios a serem seguidos ao cruzar limites.

**Mapeando Domínios para uma Arquitetura Tecnológica**

Ao agrupar domínios, como os espaços de problema e solução se relacionam? Como lidar com preocupações de dados entre domínios? Isso nos leva às **Topologias de Domínio**, que descrevem como os domínios são agrupados e inter-relacionados. As topologias exploram o equilíbrio entre governança e flexibilidade, dependência e independência, controle e autonomia.

- **Topologia de Domínio Totalmente Federada:** Exemplo da Data Mesh, fine-grained, sem orquestrador central. Distribuição peer-to-peer. Dados como produto são fine-grained, empacotando código, dados/metadados e infraestrutura. Promove especialização e reuso. Desafios: exige conformação de todos os domínios, duplicação de capacidade e utilização da rede, problemas de granularidade fine-grained para performance, qualidade e reconciliação. Difícil de alcançar, comum em organizações nascidas na nuvem com microserviços e engenheiros qualificados.
- **Topologia de Domínio Governada:** Adiciona uma camada de distribuição central para gerenciar infraestrutura. Semelhante ao modelo hub-and-spoke. Dados entregues uma vez centralmente e consumidos muitas vezes. Facilita conformação e controle em estágios iniciais. Evita tráfego de rede excessivo. Introduz acoplamento e pode ser gargalo. Difícil para implantações multicloud.
- **Topologia de Domínio Parcialmente Federada:** Menos federação, mais centralização. Mentalidade de gestão de produto de dados, mas dados em arquitetura centralizada e compartilhada. Federação no lado consumidor, centralização no lado sistema fonte. Maior autonomia e estilo mesh no lado consumidor. Aumento da sobrecarga de gestão para sistemas fonte.
- **Topologia de Domínio Alinhada à Cadeia de Valor:** Domínios trabalham em fluxo alinhado (stream-aligned), ponta a ponta na cadeia de valor. Tipicamente interconectados e operacionais. Podem ter autonomia maior dentro da cadeia de valor, com menos adesão a padrões centrais internos, mas seguindo padrões ao cruzar limites de cadeia de valor.
- **Topologia de Domínio Coarse-Grained:** Estruturas grandes, domínios alinhados a unidades de negócio, regiões, etc.. Limites nem sempre explícitos, domínios podem se sobrepor. Pode criar conflitos políticos sobre propriedade de dados. Desafios na calibração de plataformas e distribuição de dados. Implementações podem usar data warehouses coarse-grained com modelos canônicos. Exige forte governança e capacidades self-service ao longo do tempo. Risco de criar silos maiores e obscurecer propriedade de dados.
- **Topologia de Domínio Coarse-Grained e Parcialmente Governada:** Camada central de distribuição entre limites coarse-grained. Permite controles mais relaxados dentro dos limites maiores. Exige maturidade da equipe de plataforma de dados e integração self-service.
- **Topologia de Domínio Centralizada:** Resistente à descentralização, comum em organizações sem muitos engenheiros de dados qualificados. Utiliza uma plataforma de dados central (ex: data lakehouse). Dados organizados e alinhados a domínios dentro da plataforma central. Uma camada de armazenamento lógica para distribuir dados. Segregação por storage accounts, buckets, etc.. Pode incluir modelagem de dados empresarial para consistência. Ajuda organizações menores a lidar com dados históricos, coerência, eficiência de MDM e redução de custos. Combina práticas de Data Mesh como produto de dados e propriedade orientada a domínio.

Organizações podem combinar diferentes topologias. A recomendação é definir limites fine-grained sempre que possível e desacoplar ao cruzar limites de capacidade de negócio.

As **Topologias de Zona de Aterrissagem (Landing Zone Topologies)** tratam do mapeamento de domínios para a arquitetura tecnológica subjacente na nuvem. Uma zona de aterrissagem (landing zone) é uma assinatura ou conta que contém conjuntos padrão de serviços. Pode ser genérica ou focada em dados.

- **Zona de Aterrissagem de Dados Única:** Configuração mais simples com uma zona de gestão e uma zona de dados. Recomendada para controle forte, padronização, início de implementação ou simplicidade. Todos os serviços de gestão centralizados na zona de gestão, domínios de dados na zona de dados. Objetivo: fundação para domínios organizarem dados, governança consistente, propriedade autônoma de produtos de dados e ferramentas de consumo. Exemplo com Oceanic Airlines mostrando diferentes domínios colaborando. Pode escalar adicionando resource groups e automação self-service. Adequada para organizações menores ou projetos greenfield.
- **Zonas de Aterrissagem Alinhadas a Sistemas Fonte e Consumidores:** Utiliza duas zonas: uma para gerenciar serviços complexos no lado do sistema fonte (captura de dados) e outra padronizada para o lado consumidor (transformação em valor). Permite lidar com desafios complexos de integração na origem. Domínios podem estar em ambos os lados se forem provedores e consumidores.
- **Zona de Aterrissagem Hub:** Utiliza um hub central para conectividade e distribuição entre outras zonas. Dados fluem pelo hub, isolando domínios da distribuição direta. Mais eficiente no uso da rede (cópia única). Segue topologia governada. Hub é domínio-agnóstico, gerenciado centralmente. Pode aplicar virtualização de dados. Boa opção para controlar distribuição, gerenciar grandes volumes de dados históricos e em ambientes regulamentados.
- **Múltiplas Zonas de Aterrissagem de Dados:** Melhor para separar funções ou preocupações em maior escala. O agrupamento de domínios funcionais em zonas depende de segurança, coesão, eficiência (processamento de dados grandes), regiões e isolamento. Zona de gestão central supervisiona. Exemplo com Oceanic Airlines usando zonas regionais.
- **Múltiplas Zonas de Aterrissagem de Gestão de Dados:** Para grandes empresas globais com requisitos de gestão de dados contrastantes entre diferentes partes da organização. Cada zona introduz sobrecarga, decisão consciente necessária.

Ao escolher uma topologia, é importante considerar as condições da organização. Organizações menos maduras podem começar centralizadas e evoluir para federação. Grandes organizações podem explorar as diferentes topologias. O design da landing zone deve ser evolutivo, transparente, com guardrails e focado em automação.

**Gestão de Produtos de Dados**

O conceito de **"dados como produto" (data as a product)** refere-se à mentalidade de que os proprietários de dados e equipes de aplicação devem tratar os dados como produtos que são responsáveis por entregar, focando na experiência do consumidor. Um **produto de dados (data product)**, por outro lado, aborda a **arquitetura**. Há diferentes interpretações, sendo uma comum que inclui dados, metadados, código e infraestrutura. O autor argumenta que essa visão é ingênua e problemática devido à complexidade de empacotar metadados (linhagem, qualidade, segurança) e gerenciar duplicações. Propõe que produtos de dados sejam vistos como **entidades lógicas** gerenciadas separadamente da arquitetura tecnológica subjacente. Isso permite custo-benefício, distribuição de dados semânticos idênticos e evita acoplamento entre dados e metadados. Entidades lógicas podem ser descritas, classificadas e relacionadas a domínios, proprietários, etc., com elementos de dados ligando a dados físicos e metadados técnicos.

**Padrões de Design de Produtos de Dados:** Tratar dados como produto significa criá-los otimizados para leitura e fácil uso.

- **CQRS (Command Query Responsibility Segregation):** Padrão de design de aplicação que separa operações de escrita (comandos) de leitura (consultas) usando modelos de dados diferentes. Permite otimizar o banco de dados de leitura para performance intensiva, inspirando a arquitetura de produto de dados como uma cópia otimizada para leitura.
- **Design Orientado a Recursos Otimizado para Leitura (Resource-Oriented Read-Optimized Design):** Agrupar dados logicamente em torno de áreas temáticas, como em APIs REST orientadas a recursos. Produtos de dados devem ser pré-computados, desnormalizados e materializados para simplificar e acelerar o consumo. Exige traduzir modelos físicos complexos em datasets reutilizáveis.
- **Dados de Produto de Dados são Imutáveis:** Garantem que os mesmos dados possam ser regenerados. A verdade só pode ser modificada na aplicação fonte dourada.
- **Usando a Linguagem Ubíqua:** O contexto do domínio determina o design do produto de dados. Publicar contexto e definições claras em um catálogo de dados ajuda a entender a origem e o significado dos dados.
- **Capturar Diretamente da Fonte:** Capturar dados do sistema de origem para garantir uma fonte de verdade. Produtos de dados pertencem a um único domínio e não devem encapsular dados de outros domínios.
- **Padrões Claros de Interoperabilidade:** Padronizar especificações de interface (banco de dados, arquivo, API, stream) é crítico para facilitar o consumo.
- **Sem Dados Brutos:** Produtos de dados são o oposto de dados brutos; devem encapsular sistemas legados e dados técnicos. Expor dados brutos exige retrabalho pelos consumidores.
- **Não se Conformar aos Consumidores (Don't Conform to Consumers):** Os produtos de dados devem ser otimizados para um público amplo e não para um consumidor específico, que deve adaptar os dados às suas necessidades. Lógica de negócio deve ficar no domínio de origem.
- **Valores Faltantes, Padrões e Tipos de Dados:** Fornecer orientação clara sobre como lidar com dados faltantes ou de baixa qualidade e garantir formatação consistente.
- **Consistência Semântica:** Produtos de dados devem ser semanticamente consistentes em todos os métodos de entrega (batch, evento, API).
- **Abstrair Dados de Referência Voláteis:** Abstrair valores de referência complexos para valores mais genéricos, especialmente se a taxa de mudança no provedor for alta.
- **Dados Novos Significam Nova Propriedade:** Dados criados por transformação de negócio são novos e de propriedade do domínio criador. Devem seguir os princípios de distribuição. Risco de rastreabilidade exige catalogar aquisições e transformações.
- **Padrões de Segurança de Dados:** Definir orientação sobre encapsular metadados para filtragem, uso de tags/classificações para mascaramento dinâmico e uso de tabelas separadas para segurança coarse-grained.
- **Estabelecer um Metamodelo:** Criar um metamodelo consistente no catálogo de dados para definir como entidades (domínios, produtos, proprietários, termos, dados físicos, metadados) se relacionam. Isso permite a aplicação de políticas e automação.
- **Permitir Self-Service:** Construir um marketplace de dados e oferecer capacidades self-service para descoberta, gestão, compartilhamento e observabilidade. Automatizar registro via APIs e pipelines CI/CD.
- **Consistência Corporativa:** Introduzir alguma consistência fornecendo orientação para incluir valores de referência empresarial e identificadores mestres.
- **Historização, Redistribuições e Sobrescritas:** Manter dados históricos no contexto original e fornecer orientação (ex: slowly changing dimensions para dados mestres).
- **Capacidades de Negócios com Múltiplos Proprietários:** Definir metodologia para lidar com dados compartilhados (dividir produtos, encapsular metadados para segurança/filtragem).
- **Modelo Operacional:** Sucesso exige equipe dedicada (engenheiros, modeladores, cientistas) apoiada e financiada, com corpo organizacional central para padrões e melhores práticas.

A arquitetura de produto de dados ideal encontra equilíbrio entre centralização e descentralização. Padrões centralizados e governados com múltiplas instâncias de plataforma/landing zones compartilhando infraestrutura padrão reduzem custos e melhoram controle. A arquitetura deve focar em captura, transformação em versões otimizadas para leitura e serviço seguro. Componentes mínimos: capacidades de captura/ingestão (input ports), serviço/fornecimento (output ports), metadados, engenharia de dados, qualidade/MDM, marketplace. Um design lógico envolve camada de metadados, arquitetura de produto de dados por domínio e pipelines. Padronizar componentes centrais domain-agnostic abstrai problemas repetitivos.

**Capacidades de Captura e Ingestão:** Escolher método (batch vs. real-time/evento/API) baseado na velocidade e necessidade de reconciliação. Capturar metadados de linhagem e integrá-los é crucial. Gestão da qualidade de dados na origem é vital.

**Historização de Dados:** Gerenciar dados históricos é crítico (tendências, previsões). Abordagens: point-in-time (snapshots imutáveis) ou interval (slowly changing dimensions). SCds exigem mais gestão e processamento. Pode-se misturar abordagens (snapshots + "historial data as a service"). Estratégia de historização depende de tipo de dado, requisitos e regulações. Exige scripts, serviços padrão (CDC) e camadas.

**Design de Solução:** Implementar abstrações otimizadas para leitura sobre dados de aplicação complexos. Orquestração e automação de workflows são necessárias. Usar ferramentas de "modern data stack". Metadados (catálogos, linhagem) geralmente externos. Arquitetura de produto de dados pode usar tecnologia de data lake. Autonomia total na escolha de tecnologia leva a proliferação e dificuldades de governança/interoperabilidade.

**Exemplo do Mundo Real (Azure Lakehouse):** Usar landing zone de dados com serviços padrão (ADF, ADLS, Databricks) e resource groups por domínio. Ingerir para Bronze, transformar via Silver/Gold (usando dbt) para datasets consumableis (Delta). Agrupar dados por área temática. Publicar via função declarativa no catálogo (Microsoft Purview) como parte de pipelines CI/CD. Acesso via self-service criando ACLs (ou vistas seguras). Escala adicionando mais arquiteturas de produto de dados por domínio. Governança, self-service e segurança suportados por BD SQL, Functions e Purview.

**Alinhamento com Storage Accounts:** Decidir se cada data product/dataset tem conta dedicada (mais flexibilidade, isolamento, performance previsível) ou se múltiplos compartilham (facilita segurança, eficiência, processamento de dados grandes, governança estrita). Pode usar abordagem híbrida (local para processamento interno, central compartilhado para distribuição).

**Alinhamento com Data Pipelines:** Decidir se um pipeline dedicado por produto de dados ou um pipeline por aplicação que gera múltiplos produtos de dados. Pipeline por aplicação é melhor para evitar repetição e garantir integridade.

**Capacidades de Serviço de Dados:** Fornecer dados via output ports. Design depende de casos de uso (batch vs. real-time). Não confundir gestão de produto de dados com criação de valor (BI/ML). Múltiplos endpoints/portas de saída devem estar sob mesma governança e usar linguagem ubíqua.

**Serviços de Serviço de Dados:** Suportar variedade de padrões de acesso (querying ad hoc, reporting, camadas semânticas, ODBC, wrangling, data science). Armazenamento distribuído é cost-effective mas lento para queries ad hoc; usar SQL query engines ou camadas de virtualização. Virtualização boa para reporting simples, mas tem desvantagens.

**Serviço de Manipulação de Arquivos:** Construir serviço para mascarar dados sensíveis antes de distribuir arquivos planos. Conectado ao modelo de segurança central.

**Orquestração Distribuída:** Padronizar suporte para implementação, teste e orquestração de pipelines (DataOps).

**Considerações de Uso Direto:** Decidir se produtos de dados são fontes diretas ou se consumidores precisam extrair/duplicar. Duplicação pode ser necessária para criar novos dados, reduzir latência local, melhorar user experience com transformações pesadas, ou quando acoplamento com produto de dados é arriscado. Exige controle de arquitetura para evitar cópias não controladas.

**Começando:** Transição cultural e técnica desafiadora. Começar pequeno com equipes engajadas. Definir produto de dados consistentemente. Introduzir governança lentamente, focando em propriedade, metadados, interoperabilidade. Primeiro produtos de dados devem ser exemplo. Plataforma inicialmente suporta poucos tipos de ingestão/consumo. Permitir centralização inicial e alinhar engenheiros gradualmente. É uma calibração constante.

**Serviços e Gestão de APIs**

A maioria das organizações busca um **Data and Integration Mesh**, uma abordagem unificada para gerenciar planos operacional e analítico, devido aos benefícios de alinhamento. Os planos contrastam (comandos/real-time/pequenos dados vs. leitura/análise complexa/grandes dados), mas compartilham modelo de domínio, tratamento de APIs/dados como produtos e limites entre aplicações/domínios. A shift para aplicações loosely coupled e equipes autônomas em API management é similar ao Data Mesh.

**SOA (Service-Oriented Architecture):** Abordagem moderna para desenvolvimento e conectividade de software. Desacopla e define limites claros. Encapsula funcionalidade e dados via interfaces de serviço bem definidas. Padrões comuns: request/response síncrono. Recursos (REST) e operações (CRUD). Provedor de serviço (oferece funcionalidade/dados), consumidor de serviço (consome).

**EAI (Enterprise Application Integration):** Utiliza middleware para integrar aplicações heterogêneas. Patterns: broker (middleware central), hub-and-spoke, ESB (Enterprise Service Bus). ESB é middleware centralizado com capacidades de roteamento, transformação, orquestração, adaptação.

**Orquestração de Serviço (Service Orchestration):** Gerenciar fluxo de dados/serviços chamando serviços em sequência. Pode ser orquestração fina (entre componentes de app) ou orquestração de processo (BPM). Orquestração dentro de uma aplicação (fina) é menos eficiente entre aplicações. BPM gerencia workflows complexos e de longa duração, geralmente assíncronos.

**Serviços Públicos e Privados (Public and Private Services):** Classificar serviços por objetivo. Públicos (business services): entregam funcionalidade de negócio, operam entre limites de aplicação. Privados (technical services): para uso interno da aplicação, alto acoplamento.

**Modelos de Serviço e Modelos de Dados Canônicos:** Modelos de serviço descrevem interfaces (dados, entidades, relacionamentos, atributos) para facilitar uso e integração. Modelos canônicos buscam alinhar e padronizar modelos de serviço em nível corporativo (global schema). Podem se tornar grandes e complexos, genéricos demais para serem úteis.

**Modelo de Responsabilidade Federada:** API management evoluiu para um modelo mais federado. Desencoraja ESB centralizado; agregação/orquestração permitidas apenas dentro dos limites do domínio. Impacta design de APIs: princípios de design user-friendly de produtos de dados se aplicam (otimizados para leitura, linguagem de domínio).

**Princípios Adicionais de API Management:** Expor funcionalidade/dados usando modelo REST resource. Otimizar perto dos dados. Manter lógica de domínio no domínio. Construir serviços com granularidade adequada. Simplificar e usar padrões modernos. Permitir orquestração apenas dentro do domínio. Usar identificadores de domínio consistentemente.

**Arquitetura Orientada a Recursos (ROA):** Conjunto de diretrizes para arquitetura RESTful. Dados modelados em coleções de áreas temáticas homogêneas (recursos), identificados por URI. Orientado a substantivos, autodescritivo.

**API Gateway:** Componente de integração leve que gerencia, controla, protege APIs. Geralmente parte de software de API management (developer portal + API gateway). Pode ser implantado centralmente ou em múltiplos locais.

**API como Produto (API as a Product):** Domínios devem ver e gerenciar suas APIs como produtos. Exige reflexão sobre como serviços/dados são expostos.

**Serviços Compositios (Composite Services):** APIs que executam série de requests em uma única chamada. Usadas para agregar dados/funcionalidade. Ao construir, empurrar lógica de integração/transformação para consumidores.

**Descoberta de API (API Discoverability):** Recomendar manter registro de serviços (service registry, developer portal). Armazena info crítica (propriedade, versionamento, uso). Promove reuso. Ferramentas de API management (comerciais/open source). Usar formato OpenAPI para descrever APIs. Importante rastrear APIs em ambiente distribuído.

**Microserviços:** Padrão útil para quebrar aplicações em partes menores. Sinônimo: serviços loosely coupled.

**Service Mesh:** Gerencia comunicação service-to-service dentro de limites internos (não externos). Similar a API gateway (desacoplamento, monitoramento, descoberta, segurança).

**Limites de Domínio de Microserviços:** Importante definir limites lógicos (contextos limitados) em torno de microserviços. DDD usado para definir limites. Pode levar a desacoplamento em dois níveis: interno (service-to-service) e externo (domain-to-domain). APIs internas podem ser mais técnicas, externas otimizadas para consumidores.

**APIs de Experiência (Experience APIs):** APIs que atendem especificamente às necessidades de front-ends (web/mobile). Podem agrupar APIs downstream e aplicar lógica específica de front-end. Gerenciadas perto do usuário final.

**GraphQL:** Serviço para fornecer experiência de API uniforme. Linguagem de query para selecionar campos específicos de múltiplas APIs. Boa solução para extração real-time de dados de diferentes sistemas. Permite schema federation (organizar schemas como um grafo). Pode ser implantado no domínio ou para comunicação entre domínios.

**Backend for Frontend (BFF):** Camada adicional para processar lógica de interface do usuário, otimizada para front-ends específicos. Pode utilizar GraphQL.

**Exemplo Prático de Arquitetura de API:** Arquitetura com domínios A, B, C usando API gateways para publicar APIs sob controle de governança. Domínios A, B, C podem consumir APIs de outros domínios via API gateways. Domínio D consome APIs de A, B, C; pode ter camada Experience API. API gateways podem ser self-hosted ou gerenciados.

**Metadados em API Management:** Essenciais para descoberta e reuso de APIs. Publicar APIs em repositório central/portal do desenvolvedor. Usar padrões como OpenAPI. Metadados também para microserviços (propriedade, função, dados). Conectar diferentes capacidades de integração a repositório central.

**APIs Orientadas a Leitura Servindo Produtos de Dados:** APIs podem ser usadas em arquitetura de produto de dados para acesso real-time (CQRS). API gateway como roteador (comandos vs. queries). Design alternativo: microserviços orientados a leitura agindo como caches de produtos de dados, servindo via APIs. CQRS melhora escalabilidade e performance para cargas de leitura intensivas.

**Gestão de Eventos e Notificações**

Arquiteturas event-driven são o novo paradigma para experiências real-time. São complexas, requerem muitos serviços e não são solução para todos os problemas. Concentra-se em comunicação assíncrona, que se sobrepõe a API management e arquitetura de produto de dados.

**Introdução a Eventos:** Dados de evento (event data) são info sobre mudança em um ponto no tempo. Permitem reagir imediatamente (ex: detecção de fraude). Fontes variam (IoT, cliques, sistemas operacionais). Comunicação assíncrona (event-driven) é diferente de síncrona (request/response). Assíncrona: você escuta, não fala. Desafios: gerenciamento de estado, ordenação, resiliência, garantias de entrega (at least/at most/exactly once).

**Notificações vs. Carried State:** Eventos podem carregar estado completo (event-carried state transfer), permitindo que consumidor construa cópia local. Alternativa: apenas enviar notificação com referência à mudança. Notificações visam fazer consumidor reagir.

**Modelo de Comunicação Assíncrona:** Inverte a natureza conversacional. Você publica/emite, outros consomem/escutam. Produtor não sabe quem consome. Exige infra de mensagens robusta (message queues, event brokers). Plataformas de streaming de eventos (Kafka, Kinesis, Event Hubs) combinam capacidades de message queueing e event broker.

**Estilos de Processamento de Eventos (Event Processing Styles):**

- **Simple Event Processing (SEP):** Processar eventos relacionados a mudanças diretas, sem dependência de eventos passados.
- **Stream Processing (SP):** Análise contínua de fluxo de eventos em tempo real.
- **Complex Event Processing (CEP):** Analisar padrões em múltiplos eventos ao longo do tempo para identificar situações significativas. CEP é subconjunto de SP.

Arquitetura event-driven exige componentes para criação, coleta, transformação, armazenamento e consumo de eventos.

**Provedores de Eventos:** Sistemas/aplicações que geram eventos. Eventos gerados por aplicação via APIs (real-time, lightweight). Desafiador para apps legadas. Eventos gerados por banco de dados lendo mudanças (polling, triggers, CDC). Mais relacionado a event-carried state transfer. CDC lê log de transação, menos impacto no DB, mas requer ferramentas. Capturar de DBs operacionais complexos exige transformações/abstrações.

**Consumidores de Eventos:** Variações ditadas por casos de uso. Uso mais simples: desacoplar 1 provedor de 1 consumidor via fila/broker. Casos de uso: website activity tracking, data replication (cópia local para consumo offline), real-time analytics.

**Arquitetura Event-Driven em Escala:** Conectar diferentes domínios. Exemplo: arquitetura serverless com serviços de streaming, funções para processar/rotear, brokers para consumo downstream. Funções podem validar/reestruturar mensagens. Pode ser metadata-driven (config store para roteamento/mapping). Schema registry para governança e validação técnica.

**Criação de Produtos de Dados via Eventos:** Persistir eventos para combinar com processamento batch. Exemplo: adicionar arquitetura de produto de dados. Opções: capturar eventos na ingestão (útil para integrar com dados de outras velocidades, análise ad hoc/debug). Segunda opção: captar na camada de consumo (dados já modelados para consumidor, updates mais frequentes de produtos de dados).

**Governança e Self-Service em EDA:** Importante diferenciar fluxos de eventos internos (entre componentes da mesma aplicação, podem ser técnicos, metadados menos rigorosos) de fluxos domínio-a-domínio (business streams, otimizados para negócio, desacoplados, metadados mais ricos).

**Práticas Adicionais para EDA:** Event streams podem ser usados como transaction logs (audit, debug, replay). Retenção de dados em streams (custo, regulação). Padrões de resiliência (dead letter queues, idempotência, command sourcing). Consistência: eventual vs. forte. Metadados essenciais para self-service (resetar offsets, provisionar filas/streams, políticas de segurança).

**Conectando os Pontos**

Capítulos anteriores cobriram padrões de distribuição e integração de dados. Há grande sobreposição entre API management e data product architecture, pois a arquitetura interna de aplicação é a mesma. Dados, APIs e eventos herdam linguagem do mesmo contexto de domínio. Desacoplamento interno e externo é crucial.

**Distribuição de Dados vs. Integração de Aplicações:** Tradicionalmente focam em preocupações diferentes (disponibilizar dados vs. integrar aplicações). Gerenciados por DataOps vs. DevOps. Padrões dependem de caso de uso (batch vs. real-time, necessidade de reconciliação).

**Consistência e Descoberta:** Capturar contexto e significado é importante devido à confusão semântica entre domínios. Três fases de design: conceitual (entidades de negócio), lógica (representação da aplicação/DB), física (implementação real). Idealmente, camadas se ligam. Provedores de dados expõem subset do contexto do domínio, consistente nas camadas subjacentes. Modelos lógicos em ferramentas de modelagem. Modelos físicos/interfaces em ambientes de criação; escanear para metadados técnicos. Integrar metadados complexo. Metadados são cruciais para integração/consumo inteligentes.

**Recomendações para Descoberta e Semântica:** Escolher metodologia de modelagem de dados (catalog, comunidade). Qualidade de metadados (validar ligação a termos de negócio). Metamodelo de gestão de dados (entidades, relações, tooling). Segurança/privacidade (classificar, taggear, políticas). Comunicar e ser transparente. Taxonomia clara para elementos do modelo. Definir estratégias para modelagem, descoberta, consistência semântica, governança, controle, manutenção.

**Inspirando, Motivando e Guiando a Mudança:** Suportar equipes de domínio: Playbook com padrões (características, tecnologia, prós/contras). Lista oficial de serviços/produtos aprovados. Ajustar processo de controle de arquitetura (template de solução com dados/integração). Centro de competência (coaching, docs, exemplos). Considerar efeitos a longo prazo de decisões de curto prazo. Lidar com exceções (casos críticos de latência).

**Estrutura Organizacional e Funções:** Implementação de arquitetura exige planejamento, execução, coordenação e mudança organizacional. Atores típicos em projeto de dados: equipes de domínio (business owner, application owner, dev, eng de dados), equipe de plataforma de dados (fornece capacidades self-service), equipe de infraestrutura (fornece infra genérica). Equipe de governança de dados garante conformidade. Capacidades da equipe de plataforma de dados: segurança, reporting, governança, integração/distribuição, analytics. Podem se tornar domínios próprios. Funções como segurança/operações cruzam camadas.

**Team Topologies:** Abordagem para organizar equipes para fluxo rápido. Tipos: stream-aligned (equipes de domínio, focam em valor), platform (fornece produto interno para acelerar stream-aligned), enabling (ajuda stream-aligned a superar obstáculos), complicated subsystem (gerencia partes complexas). Mapeamento: equipes de domínio = stream-aligned; equipe de plataforma/infra = platform; áreas de expertise (integração complexa, segurança, governança, MDM, qualidade, consumo/analytics) = enabling/complicated subsystem. Equipes enabling suportam e treinam.

**Data Governance e Data Security**

Governança de dados determina o que os dados representam e para que podem ser usados; segurança garante que apenas partes autorizadas acessem dados. Exige padronização, metadados, atribuição de proprietários, classificações e contratos.

**Data Governance:** Necessária em organizações grandes devido à escala e complexidade. Não deve ser percebida como burocrática, mas sim habilitadora.

**O Framework de Governança:** Exige repensar cultura, design, modelo operacional, funções. Funções comuns: Proprietário de Dados (responsabilidade final), Curador de Dados (gerencia metadados), Conselho de Governança de Dados (toma decisões, resolve conflitos), Consumidor de Dados (utiliza dados), Criador de Dados (produz dados). Funções federadas em grandes empresas (cada domínio tem proprietário, stewards). Framework formal documenta funções, responsabilidades, políticas, padrões, procedimentos. Exemplo de framework por atividades na jornada de dados. Modelo de maturidade (níveis 0-5). Deve ser customizado e focar no lado organizacional.

**Corpo de Governança (Data Governance Team):** Garante implementação do framework, define o que é boa governança nos domínios (shift cultural), foca em coaching/treinamento.

**Processos (Atividades de Governança de Dados):** Definir processos para catálogo, propriedade, regulamentação (PII), qualidade, linhagem, classificação, impacto de regulação, alinhamento com arquitetura, MDM, ciclo de vida, schemas/padrões, metadados, cultura de dados, retenção, monitoramento, modelagem, princípios de onboarding/consumo, auditoria, roundtables. Selecionar subconjunto para início.

**Tornando a Governança Efetiva e Pragmática:** Garantir buy-in da alta gerência. Governança facilita tarefas (mitiga riscos, self-service). Confiar nas equipes locais. Focar na qualidade upstream (na origem). Ser realista. Focar em consumidores e casos de uso. Governança é contínua, automatizar (computacional). Equipe de governança com engenheiros qualificados para automação. Ser rigoroso em princípios centrais (captura da fonte, propriedade, acesso via contrato). Compartilhar progresso (transparência, comunicação). Medir sucesso por valor de negócio. Encontrar equilíbrio, fazer piecemeal. Comunicação consistente (linguagem, benefícios). Gerenciar metadados de forma inteligente (automático, padronização). Fazer parte da cadência de planejamento/execução.

**Serviços de Suporte para Governança de Dados:** Transparência e confiança exigem metadados centrais. Ferramentas: Catálogo de Dados (inventário de metadados), Repositório de Aplicações (info sobre apps, proprietários), Lista de Elementos de Dados Únicos (visão geral de dados, propriedade, classificações), Aplicação de Contrato de Dados (DCA) (registra contratos entre partes), Repositórios de Metadados (DBs/ferramentas para gerenciar metadados).

**Data Contracts:** Acordo formal entre provedor e consumidor sobre quais dados podem ser consumidos e para quais fins. Importantes para propriedade, padrões, gestão de pipelines, uso de dados. Cobrem preocupações técnicas (compatibilidade, SLO) e de negócio (propósito, privacidade). Exemplo de registro de metadados para contrato de dados. Podem ser armazenados separadamente em repositório central. Padronizar pipeline framework e metadados de contrato. Alinhar atributos de contrato com catálogo de dados (termos de negócio). Acordos éticos/distribuição difícil automatizar.

**Melhores Práticas para Começar com Contratos de Dados:** Lento, cultural, não complicar, modelo simples (formulário), aceitar processos manuais. Focar em propriedade/estabilidade primeiro. Iterar metadados. Adicionar automação (validar via testes, webhook). Implementar workflow de aprovação (proprietários/stewards no lead, governança supervisiona). Melhorar segurança (seleções fine-grained, mascaramento dinâmico). Fase final: self-service total. Contratos de dados são novos, focam em estabilidade técnica, automatizar gradualmente.

**Data Security:** As implementações atuais focam em silos (data warehouses/lakes). APIs/Eventos usam modelos diferentes focados em endpoints. Modelo data-centric deve focar em trust boundaries e metadados de governança.

**Trust Boundaries:** Nível de confiança muda ao mover dados entre domínios. Mais difícil entender riscos, amplificação de riscos de privacidade. Gerenciar segurança nas bordas (interfaces de produtos de dados, APIs, eventos). Ações recomendadas: esquema de classificação/propósito, contratos de dados para cruzamento de limites, monitoramento de movimentação. Exige classificação consistente, esquema de uso de dados e modelo de segurança unificado.

**Classificações e Rótulos de Dados:** Clusterizam dados para uso/proteção eficiente. Nível avançado para segurança/compliance. Lista mantida centralmente, não muito longa. Classificações input para políticas de controle automático. Importante classificar todos os elementos de produtos de dados. Rótulos desacoplam regras da dados.

**Classificações de Uso de Dados:** Limitam uso a cenários/casos específicos (DATA_SCIENCE, AD_HOC_USAGE, HR_ONLY, etc.). Afetam o que usuários podem ver/fazer. Podem ser combinadas com classificações de dados.

**Segurança de Dados Unificada:** Incorporar requisitos de segurança nas plataformas/serviços desde o início. Dois níveis de controle: dados (acesso, mascaramento, criptografia, monitoramento, identidade) e infraestrutura (isolamento, rede, firewall). Foco no nível de dados.

**Modelos de Controle de Acesso a Dados:** ACLs (Access Control Lists). RBAC (Role-Based Access Control): restringe acesso por papéis e permissões; risco de "explosão de papéis", estático. ACLs melhores para nível de usuário/dados baixos; RBAC para organização. ABAC (Attribute-Based Access Control): método avançado, políticas combinam atributos (dados, sistema, usuário, ação). Arquitetura ABAC: PEP (impõe política), PDP (decide política), PIP (fornece info externa), PAP (gerencia políticas).

**Identity Providers (IDPs):** Fontes externas para recuperar atributos de usuário (AD, OpenLDAP). Autenticação: SAML, OpenID Connect. OAuth para autorização. SSO (Single Sign-On).

**Exemplo do Mundo Real (Arquitetura de Segurança):** Baseado em arquitetura de produto de dados. Componentes de metadados e segurança no topo (Metadados, Segurança) e componentes de acesso virtualizado embaixo (Aplicação de Produto de Dados). Metadados da arquitectura superior controlam acesso aos dados. Metadados de negócio/dados técnicos no catálogo. DCA, PIP, PAP interagem para decidir acesso. Framework de ingestão metadata-driven atua como PIP. Storage account com contêineres por domínio/produto de dados. Pipeline de dados orquestra passos, usa DCA/Great Expectations para validação. Componente de enforcement permite acesso/queries, loga acesso.

**Fluxo Típico do Processo de Segurança:** Onboarding de dados/metadados. Registro no catálogo, scan, profiling, classificação, rotulagem automática. Preservar contexto de segurança de domínio local. Certificar dados. Pedido de acesso: consumidor pede dados via marketplace/catálogo. Criação de contrato de dados (ACD) com aprovação (workflow). Coleta de informações para decisão (PIPs fornecem atributos de consumidor, dados, aplicação, contexto externo). Combinar info e submeter ao PDP para política. Início gradual com decisões simples. Alternativa: grupos de segurança e gerenciamento de entitlement (Access packages).

**Aplicação de Políticas de Segurança:** Após decisão, políticas aplicadas em camada de acesso/computação. Pode envolver gerar statements SQL (RLS/CLS), Apache Ranger, ferramentas de terceiros. Ou duplicar/de-identificar dados por caso de uso (Privitar). Combinação de abordagens possível. Ex: Vistas seguras (filtra/mascara/embaralha dados). Enforcement real-time via ferramentas. Acesso temporário/limitado para fins específicos. Duplicação/pré-processamento para datasets grandes. Processo cíclico quando consumidor se torna provedor. Construir arquitetura de segurança lentamente. Focar na segurança intra e inter-domínio. Automatizar o máximo possível.

**Propriedade de Dados Compartilhados:** Lidar com capacidades de negócio compartilhadas. Métodos: dividir dados fisicamente em múltiplos produtos (melhor para propriedade coarse-grained), encapsular metadados para segurança (melhor para segurança fine-grained, muitos proprietários). Modelo de delegação de propriedade para aprovações.

**Segurança de Arquiteturas Baseadas em API:** Segurança em duas camadas: API (API gateways, ESB, service meshes) e provedor de API. Camada API: autenticação (tokens), API gateway valida escopo/token. Políticas API originam de DCA. API gateway é PEP. Pode usar PIPs externos via mapping templates/microservices. Provedor de API pode usar DCA como PDP. Seguranca em APIs compostas: provedor original mantém regras locais, serviço agregador mantém regras gerais (não permitir combinação específica). Consumidor fornece contexto para autorização.

**Segurança de Arquiteturas Event-Driven:** Mais complexo, poucos produtos/padrões. Abordagem: metadados de segurança (classificações, rótulos) e políticas baseadas em atributos. Eventos em tópicos internos, depois filtrados por política em microserviços e provisionados em tópicos específicos do consumidor. Pode juntar dados de outros contextos para decisões de política.

**Democratizando Dados com Metadados**

Metadados e IA intersecarão, impactando a integração de dados. Metadados descrevem aspectos da nova arquitetura, ligam tudo, chave para insights, controle, eficiência. São complexos de gerenciar (espalhados, acoplados a vendors), exigem seleção, organização, integração.

**Categorias de Metadados:** Negócio (governança, descoberta, compreensão), Técnico (estrutural, design time), Operacional (processamento, runtime), Social (usuários, uso). Fundacionais para Data Fabric, governança suportada por AI.

**Metadados Management:** Conceito antigo, termo cunhado nos anos 90. Ferramentas iniciais focadas em DBs, modelagem forward/reverse. Cenário moderno: metadados espalhados, silos por plataforma/ferramenta. Dificulta visão geral. Exige padronização, escolhas cuidadosas, procedimentos. Objetivos: Modelo de Metadados Empresarial (MME), Coleções Críticas/Metalake, Democratização (Knowledge Graphs, de metadados.

**Exemplo Prático de um Metamodelo:** Implementar metamodelo em solução de governança (ex: Microsoft Purview). Blueprint/mapa com entidades lógicas e relações. Ex: produto de dados relacionado a domínio e dataset físico. Instanciar entidades lógicas ("customer dataset") e definir relações. Metamodelo deriva do MME, mas pode ser diferente na implementação (subconjunto, restrições da ferramenta).

**Domínios de Dados e Produtos de Dados (no Metamodelo):** Coração do MME. Produtos de dados: entidades lógicas (valor significativo para gerenciar/governar). Ligados a objetos físicos, aplicações, processos, proprietários, termos, etc.. Domínios de dados: limites onde dados são gerenciados, alinhados a value streams/capacidades. Pessoas, processos, tecnologia se unem. Domínios também têm muitas relações.

**Elementos de Produto de Dados:** Entidade lógica (opcional no metamodelo) para ligar a termos de glossário e atributos técnicos. Opções: considerar termo de glossário ligado a atributo técnico como elemento, ou criar entidade separada "data product element".

**Modelos de Dados (no Metamodelo):** Exigem atenção especial (tipos, níveis, maturidade, complexidade, relações).

- **Modelos de Dados Conceituais:** Nível mais alto (folksonomies, ontologies, taxonomies). Resumem conceitos de negócio críticos, significado dos dados. Enriquecer termos com definições (domain-specific), atributos, dependências. Podem incluir conceitos futuros ou sem implementação. Não limitados a um domínio. Relação glossário-atributos geralmente um-para-muitos; muitos-para-muitos para capacidades compartilhadas/MDM [524em ser recuperados de ferramentas de modelagem.
- **Modelos de Dados Físicos:** Descrevem design implementado. Podem ser documentados manualmente ou por scan de fontes (metadados técnicos). Design físico pode ser diferente do lógico por performance/segurança.

**Limitações e Melhores Práticas (Modelos de Dados):** Ferramentas não cobrem todos os serviços/tipos de DB. Scanners descobrem metadata físico, revertem para lógico/conceitual, ligam a termos. Benefício: clareza da tradução de negócio para DB. Ajuda técnicos a reagir, reusar conhecimento. Ligação metadado negócio-técnico permite integração/consumo inteligentes (ontology-based data integration). Metadado obsoleto falha ambições de data fabric.

**Data Lineage:** Origens de dados e fluxo ao longo do tempo. Conhecimento crítico para visibilidade, debugging, insights em transformação. Pode indicar impacto de qualidade na origem. Importante definir princípios de captura.

**Outras Áreas de Metadados:** Metadados de aplicação (arquitetura, componentes), Metadados de tecnologia (ferramentas, frameworks), Metadados de processo (fluxos de trabalho), Metadados sociais (uso, ratings), Metadados organizacionais (departamentos, equipes). Trazem valor, reduzem custos, ajudam em conformidade/segurança.

**A Arquitetura do Metalakealtam controle automatizado, integração com outros serviços, capacidades de descoberta inteligente. Exige entrada manual, risco de metadado obsoleto/inválido. Promove visão do vendor. Recomendações: Estratégia de metadados (agora/futuro, prioridades). Scopo, delegar propriedade, alinhar metadados técnicos/negócio com domínios. Vincular metadado técnico (fonte/produto) ao domínio. Criar glossários/hierarquias por domínio para metadado de negócio. Domínios ligam termos a atributos técnicos. Sempre começar com propriedade. Entradas especiais para dados corporativos/MDM. Ciclo de vida de metadados. Rigor e consistência com rótulos/classificações. Atenção à linhagem (padronizar ferramentas de transformação). Equilíbrio entre escanear tudo/relevante. Comunicação top-down (ambição, progresso)ologias: RDF (Resource Description Framework), RDFS (RDF Schema), OWL (Web Ontology Language), SPARQL (SPARQL Protocol and RDF Query Language), SHACL (Shapes Constraint Language), SKOS (Simple Knowledge Organization System), Thesaurus. Permitem criar knowledge graph metadata-driven enterprise-wide. DB representa conhecimento/relações da organização. Exemplo com Graphmarts e Data Layers (mapear metadados/fontes em camadas). Camadas: source (input de fontes/metadados), Data Layers (expandem source, usam SPARQL), Data Layers (projeções para consumidores, semantic layers). Relações entre camadas, receitas (transformação) em ontologias (RDF). Pode ter knowledge graphs distribuídos (domínio + central).

**Data Fabric para Metadados Management:** Expandir visão do knowledge graph para MME. MME como camada semântica sobre knowledge graph. Permite navegar, buscar, query metadados como dados. Ex: ligar classificações de privacidade a termos conceituais, saber quais atributos são sensíveis.

**Metalake Solution Design:** Espelha padrões de capítulos anteriores. Metadados validado/limpos/integrados. Reutilizar blueprints/serviços de arquitetura de produto de dados. Exemplo: Camadas (ingestão, transformação), Serviços API (comunicação frontend), Knowledge Graph (armazena metadados integrados). Catálogo e KG fortes; KG mais flexível/amplo.]. Usa catálogo, KG, outros repos de metadados. Coleta metadados via APIs. Pode ter capacidades analíticas/provisionamento. Classificar produtos de dados por KPIs (rating, qualidade, uso).

**Melhores Práticas (Marketplace):** Abordagem step-by-step. Integrar bidirecionalmente marketplace, KG, catálogo. Promover enriquecimento de metadados. Definir modelo de governança colaborativa (papéis, domínios). Definir metamodelo alinhado ao catálogo. Dashboards/métricas/alertas para qualidade de metadados. Desafio: criação de plataforma, organização, processos. Benefício: atingir metas da estratégia de dados.

**Modern Master Data Management (MDM)**

MDM é essencial em ambiente distribuído para consistência e confiança. Aplicações entrelaçadas, federação aumenta velocidade mas causa inconsistências. MDM foca em detectar e resolver inconsistar, gerenciar governança, melhorar qualidade no hub. Para analytics, BI, reporting. Não melhora dados nas fontes douradas. Similar a data warehousing.

- **Registry:** Estilo mais fácil, tabela de referência cruzada liga IDs locais a sistema de referência interno. Sem impacto nas fontes douradas, sem distribuição de volta. Sem impacto direto na qualidade da fonte.
- **Centralized:** Repositório central serve a todos os sistemas (operacionais e analíticos). Coleta, liga, limpa, match, enriquece, publica dados de volta para sistemas fonte. Preocupação: intrusão nos sistemas fonte, sincronização bidirecional difícil, tight coupling. Comercialmente defendido como ideal, mas autor aconselha cautela.
- **Coexistence:** Combina Registry e Centralized. Distribui dados mestres para sistemas que precisam, mas não força conformação de todos. Permite flexibilidade na integração.

Estilos podem permitem replay. Separar log de transação de solução MDM. Casos de uso com maior latência: distribuir via arquitetura de produto de dados. Não fazer conexões ponto a ponto entre hub MDM e apps provedoras/consumidoras. Hub MDM deve usar os mesmos produtos de dados que os consumidores usam para evitar inconsistências.

**Projetando uma Solução MDM:** Identificar dados únicos/confiáveis nas aplicações/sistemas (golden source). Entender contexto, regras de negócio, formatos de cada fonte. Projetar solução que harmonize/torne consistente. Pode ser solução vendor ou customizada. Passos de design: identificar dados mestres/referência, escopo, identificar fontes, definir modelo de dados (entidades, atributos, relações), regras de qualidade, distribuição de melhorias de volta à fonte, propriedade de dados criados, aprovação de mudanças, definir definições mestras no catálogo.

**MDM Orientado a Domínio:** Delinear claramente entre dados de referência (classificam/organizam outros dados) e dados mestres (dados core essenciais). Gerenciados e distribuídos de forma diferente.

**Dados de Referência:** Domínios podem precisar conformar produtos de dados a dados de referência empresarial centralizados. Ex: códigos de país. Isso parece contradizer Data Mesh, mas é necessário para conformação regulatória. Abordagens: Serviços centrais para domínios descobrirem/incorporarem dados de referência (catalog, storage account central).am dados mestres e dados de domínios. Críticos para rastrear o que foi "mastered" e o que pertence junto. Identificar e atribuir IDs mestres só pode ser feito centralmente. Exemplo de design de solução. IDs mestres ligam registros "master" a grupos. Chaves de aplicação vs. chaves substitutas (surrogate keys). Distribuir IDs mestres apenas para apps/sistemas sujeitos a MDM.

- dados mestre empresariais, dados mestre de domínio, dados locais. Exemplo de design: MDM dentro do domínio, sobrepõe com produto de dados. Oferecer serviços MDM as-a-service. Segregar domínios (ou grupos) na solução central.

**MDM e Qualidade de Dados como Serviço:** Oferecer capacidades MDM as-a-service para domínios. MDM complexo, abstrair infra simplifica. Segregar domínios (ou grupos) em solução central. Distinguir datasets empresariais de domínio via catálogo/metadados. Combinado com DataOps para pipelines automatizados. Overlap, mas focos diferentes (humano vs. código). Ambos buscam entregar valor e facilitar vida do consumidor. MDM: coletar, linkar, limpar, integrar, match, enriquecer, distribuir; remover duplicados, corrigir inconsistências; foco em entity consolidation, cluster reduction. Resultado MDM: fontes autormedallion".

**Criação de Dados Curados:** Processo de transformar dados (coleta, limpeza, integração, harmonização) para torná-los úteis para casos de uso específicos. Pode mudar contexto semântico, criar novos datasets.

**Alcançando Consistência Empresarial:** Várias abordagens:

- **Troca de Conhecimento (Knowledge Exchange):** Compartilhar conhecimento (metadados) entre domínios (file extra, encapsular, repositório central). Metadados incluem localização, atributos, relações, significado. Usar rótulos/anotações para equivalência. Acelerar com crowdsourcing, scanners, IA.
- **Vistas Integradas (Integrated Views):** Queries pré-definidas (vistas virtuais) que recriam dados no momento da execução. Comum em MDM e EDW. Exige modelo de dados lógico consistente.

**Republicando Dados via Hubs de Integração:** Domínios extraem para plataforma de. Modelos de colaboração: separate ways (flexibilidade), shared kernel (componente compartilhado), partnership (colaboração próxima), customer/supplier (provedor/consumidor). MDM overlap com agregados (agregação de dados para criar nova visão).

**Master Data Ágil:** Dados rápidos/fluidos, quebrar para domínios; dados estáveis/importantes, usar MDM. Linhagem ajuda a identificar o que realmente importa. Abordagem Ágil para MDM (ativos e Dados em Valor**

Foco no lado consumidor. Casos de uso exigem combinação de dados de diferentes domínios/produtos de dados. Interação complexa (técnica e organizacional). Advocacia para gerenciar dados focados no consumidor diferente de produtos de dados. Preocupação central: combinar dados para casos de uso de negócio, transformar em valor. Grande variedade de ferramentas/disciplinas/papéis/atividades no lado consumidor. Requisitos de negócio ditam ferramentas/técnicas. Requisitos não funcionais (performance, etc.) afetam resultado. Construir padrões/blocos de construção reutilizáveis.

**Os Desafios de Transformar Dados em Valor:** Usuários de negócio compram/implantam ferramentas próprias, interfaces ponto a ponto, ETL em ambientes próprios. Arquitetura se torna complexa, cara, difícil. Necessidade de ambientes controlados que integrem com serviços subjacentes, suportem modelo federado e atendam necessidades de self-service [612 alta ou múltiplos produtos precisam ser combinados. Criação de novos dados (lógica de negócio complexa) exige reter resultados.

**Domain Data Stores (DDSs):** Novo bloco de construção para gerenciar dados consumidos e recém-criados. Similar a stores de produtos de dados. Armazenam e gerenciam dados para casos de uso do consumidor. DDSs importando/armazenando dados necessários apenas quando produtos de dados não podem ser usados diretamente.icação pode ser ambos.

**Granularidade de Casos de Uso Alinhados ao Consumidor:** Definir limites claros para DDSs. Escopo, tamanho e colocação difíceis. Bounded contexts subject-oriented, alinhados a capacidades/value streams. Pode haver um DDS grande para vários casos de uso, ou um domínio grande com múltiplos DDSs. Consistência semântica importante. Subdomínio genérico pode fornecer lógica de integração reutilizável. Reg de domínio (Bronze/Silver compartilhados, Gold específico). Layering em DDSs tem escopo claro. Guiar lado consumidor é chave para federação. Gerenciar DDSs em conjunto com produtos de dados/MDM.

**DDSs vs. Produtos de Dados:** Objetivo DDS: consumir dados, criar valor (decisões, reporting, ML). Objetivo Produto de Dados: fornecer dados. Dados em DDSs estruturados/otimizados para caso de uso; produtos de dados otimizados para leitura9, 620, 633]. Princípios diferentes para cada tipo. Dados integrados não devem ser compartilhados diretamente. Distribuir dados integrados exige aprovação do(s) proprietário(s) dos dados originais. Copied data também exige rastrear linhagem. Lidar com preocupações de integração sobrepostas (MDM, agregações). Escopo/desacoplamento se aplicam a serviços analíticos/BI. Dados integrados distribuídos criam novas dependências7]. Brainstorming com equipes, clusterizar output. Business model canvas útil. Requisitos determinam quantidade de dados. Clarificar objetivos (problemas, fontes, operacional, real-time/offline, criticidade, reuso).

- **Público-Alvo e Modelo Operacional:** Determinar quem é necessário e responsabilidades. Papéis (proprietário de domínio/aplicação, analista/SME, cientista de dados, construtor de relatório) [640, 641ência, leitura/escrita, dados históricos, otimização integridade/performance, restrições do vendor. Considerar custo, open source, features, integração, segurança, privacidade, governança, desenvolvimento. Oferecer conjunto limitado e reutilizável de tecnologias de DB como blueprints.
- **Data Pipelines e Modelos de Dados:** Processamento de dados (batch, stream). Ingestão real-time: capturar/armazenar mensagens, buffer, integração da plataforma de reutilizáveis. Separar input/lógica/output. Versionar pipelines para reprodutibilidade. Rodar pipelines em paralelo (elasticidade). Motores de processamento distribuído (Spark). Lógica de transformação em templates/linguagem de alto nível.
- **Pipelines SQL vs. NoSQL:** SQL melhor para queries complexas, exige mais validação/estrutura. NoSQL mais simples/flexível, escalável, queries mais rápidas, mas difícil query passos. Back-pointers para produtos de dados. Nomes de arquivos, chaves, IDs, etc. em ferramentas centrais. Princípios rigorosos para captura centralizada. Restrições em ferramentas ETL.

**Escopo do Papel dos DDSs:** Persistir dados para consumo downstream.

- **Analytical Data Stores:** Integrar/combinar dados para workloads analíticas (ML). Foco: dados de alta qualidade, achatados/desnormalizados.
- de domínio. Alinhar blueprints para cenários. DDS que só distribui agregado não precisa BI/ML. Scopo dos stores ajuda a reduzir complexidade. Abordagem federada: usar blueprint genérico como ponto de partida, ajustar minimamente. Padronizar é chave.

**Zoning em Data Stores:** Separar preocupações (ingestão, limpeza, curadoria, serviço) logicamente/fisicamente (zonas: Bronze, Silver, Gold). Zonas segreg). Domínio deve escolher modelagem que melhor se adequa ao caso de uso.

**Camadas Semânticas (Semantic Layers):** Abstrair DDS com camada semântica para apresentar dados compreensíveis. Representações de negócio para usuários acessarem dados autonomamente. Geralmente parte de serviço de reporting, construídas por devs BI/usuários de negócio. Benefícios: clareza, terminologia comum, desacopla store, funcionalidades extras (versioning, segurança, monitoramento) [656, 657icas:** Overhead extra para relatórios simples. Podem se tornar caras (grandes volumes de dados). Complexidade com overlap de relatórios/modelos compartilhados. Definir limites, alinhar com capacidades. Usar modelos compartilhados apenas se estáveis/importantes. Exige clareza sobre insights, perguntas, objetivos, fontes.

**Self-Service Tools e Dados:** Ferramentas de descoberta, preparação, visualização de dados. Aceleram obtenção de valor, f661]. Segmentação por habilidades/ferramentas afeta padronização. Reduzir número de ferramentas. Distinguir dados gerenciados (necessidades contínuas, estável, automatizado, padrão, governado) de dados self-service (análise ad hoc, temporário).

**Processos Self-Service:** Começam com entender dados/relações/uso. Validar hipótese analisando/combinando/ajustando dados. Operationalizar valor: construir pipeline gerenciado (produção) [66 Alinhar propriedade/gestão com domínios. Playbooks/design patterns (carregamento, conexão). Centro de Excelência (COE): equipe central para padrões, treinamento, guidelines, suporte. Falar com stakeholders. Níveis de maturidade para guiar uso de BI (Nível 1: novo, sem disciplina; Nível 2: repetível, gerenciado; Nível 3: crítico, automatizado). Rede de campeões (compartilham conhecimento). Requer9]. Processo MLOps alinhado a federação/Data Mesh. Permite operar em escala, processo padrão ponta a ponta. Processo de referência MLOps: iniciação, experimentação, engenharia de dados, operacionalização do modelo, monitoramento de modelo. Cada fase tem estágios/passos.

**Iniciação de Projeto (MLOps):** Entender requisitos, planejar implementação. Definir objetivos, critérios de sucesso. Usar DevOps tooling ( logar melhorias, provar a reguladores. Ferramentas (MLFlow). Escolher infra/ambiente (limpo, versionar, capturar inputs/outputs). Atenção à engenharia de dados. Determinar padrão de integração em produção: Modelo como API (serviço web), Modelo como Container (executar como batch), Modelo como Função (serverless), Modelo como Database/File, Modelo**Engenharia de Dados (MLOps):** Lidar com qualidade/dados faltantes (reportar para origem). Desenvolver pipelines para automatizar retreinamento em produção. Incluir detecção de data drift e mudanças de schema. Registrar datasets em ambiente ML. Desenvolver feature store (repositório centralizado de features). Benefícios: eficiente criação de features, reuso.

**Operationalização do Modelo (MLOps):** Implementar pipeline automatizado ponta a ponta.

**Variações e Exceções (MLOps):** Pipelines com aprovação manual, parada automática por qualidade. Gatilhos de retreinamento (release, evento, dados novos). Pipeline implantação acoplado a engenharia. Dados não estruturados (serviços cognitivos, compartilhamento externo). Modelos em ambientes diferentes. Repositório central de metadados para oversight. Serviços cognitivos prontos (ChatGPT).

**Colocando aveis. Garantir compromisso top-down. Comunicar, engajar stakeholders. Identificar iniciativas, casos de uso (potencial de impacto). Alinhar com estratégia de dados/negócio. Avaliação de complexidade técnica, priorizar. Benchmark (complexidade, custo, valor, risco, gerenciamento). Selecionar candidatos (low-hanging fruit). Primeiro caso de uso: não muito difícil, entrega valor, define exemplo.lar. Determinar capacidades centrais vs. descentralizar.

**Tornando Real:** Abordagem faseada, começar pequeno. Hipótese de valor, poucos casos de uso, experiência com produtos de dados, princípios, ineficiências operacionais, "just enough" governança. Discussão segue "happy path".

**Fase Oportunista (Set Strategic Direction):** Onboarding primeiros domínios, não mapear tudo. Objetivos: aprender conceitos, traduzir prática, catálogo). Lakehouse architecture. Base para construir produtos de dados em escala, self-service, governança computacional. Equipe de domínio/plataforma colaboram. Extrair/ingerir (Bronze), transformar (Silver, Gold). Catálogo escaneia dados/integração. Tornar dados disponíveis.

**Lakehouse Architecture Inicial:** Poucos serviços essenciais. Organizar catálogo: escanear domínios do07]. Alinhar domínios de aplicação/negócio (domínio usuário cria relação).

**Características da Fase Oportunista:** Maturidade baixa, processos manuais. Pipelines manuais/fixos. Equipes de domínio registram produtos de dados no catálogo, responsáveis por metadados (com ajuda central). Single landing zone/recursos. Linhagem ausente/em ilhas. Catálogo não self-service para publicação, info limitada.]. Equipes de domínio mais proficientes, self-supporting. Produtos de dados mais complexos, governados, controlados. Automação, capacidades avançadas (framework qualidade, padrão ETL). Fornecer blueprints/padrões.

**Lakehouse Architecture Atualizada:** Adicionado ingestão real-time, framework metadata-driven, log database, acesso virtualizado. Pipelines metadata-driven e parametrizados (config runtime de DB/repo). Workflow parametrizado usa metadados [or antes do consumidor. Mais consumidores que provedores. Serviços analíticos complexos, atraem atenção. Garantir entrega estável/escalável de produtos de dados antes. Evitar que equipes repitam problemas de engenharia. Não perder confiança do negócio.

**Temas Estratégicos (Exemplo):** Gestão de Fonte Dourada (propriedade, qualidade, gaps). Gestão de Data Marketplace (criação, compartilhamento, monetização) [71 guardrails declarativos.

**Padrões/Opções de Ingestão:** Habilitar diferentes padrões baseados nas necessidades (evento, CDC, áreas de staging intermediárias). Para fontes externas/legadas.

**Situação ao Fim desta Fase:** Governança e modelo operacional definidos (papéis, responsabilidades). Dashboards mostram representação de domínios/metadados. Equipes de domínio possuem ciclo de vida de dados (fonte a produto de dados). Possvidas com self-service, automação. Guiar equipes para serem eficientes em gestão de dados/pipelines. Self-onboard, self-subscribe, self-registration/manutenção metadados (APIs integradas com CI/CD).

**Próxima Iteração da Arquitetura:** Foco em real-time, consumption-readiness, segurança, MDM, dados curados. Padronizar lado consumidor com blueprints/serviços. Uso de dados diverso, lançar um serviço novo por vez [724 aos dados próprios com governança central. Poucas soluções out-of-the-box. Framework de contrato de dados customizado para workflows e criação automática de políticas (vistas seguras). Inclui semântica, qualidade, SLOs.

**Situação no Fim desta Fase:** Governança/políticas de acesso automatizadas (workflows, API registration). Uso de dados/processos de criação de valor padronizados (blueprints). Serviços MDM adicionados (consistência pontcial envolver usuários de negócio. Devem ver valor, impulsionar criação de produtos de dados, ter propriedade, seguir padrões. Campanha, ser firme na direção certa. Se proprietários não descrevem/classificam dados, não permitir compartilhar. Se qualidade cai, bloquear distribuição. Controlar/guiar, aberto a feedback, fazer melhorias.

**DataOps:** Cultura, não apenas tecnologia. Workshop de Design de Solução (DataOps Day 0).2]. Converte raw input em informação significativa. Propriedade dos eng de dados. Longa sequência de passos possível. Exemplo de pipeline (coletar, validar, copiar, atualizar schemas, triggar qualidade/transformação, atualizar status, informar consumidores, triggar catálogo). Fatores/perguntas de design. Passos específicos vs. genéricos (monitoramento, logging). Padronizar passos genéricos. Rep governança. Definir papéis (RACI), atividades. Descrever instruções práticas (framework). Atenção à alfabetização (valor de dados, comunicação). Tornar divertido, criar entusiasmo.

**O Arquiteto Empresarial Moderno:** Não é apenas "policial". Foco: estratégia, engenharia (profundidade técnica), custódia (governança, segurança, conformidade), liderança (inspirar, guiar), pragmatismo (trade-offs), comunicação [7).

**Habilidades do Arquiteto:** Escuta e comunicação cruciais. Não sobrecarregar stakeholders com jargão. Assertivo, mente aberta, empático, pragmático. Prescritivo (guiar product owners/engenheiros).

**Controle e Governança (EA):** Equilíbrio objetivos longo prazo/praticidade. Frameworks EA tradicionais (TOGAF) não se encaixam bem no mundo DevOps/DataOps. Pint]. EA: da ideia à implementação. Esboçar visão, design abstrato, estratégia, criar energia, suportar equipes. Começa com ver valor em dados. Voar é seguro, mundo de dados é maravilhoso, estamos apenas começando.

Este resumo é uma compilação e estruturação detalhada dos principais tópicos, argumentos, conceitos, princípios e práticas abordados nos excertos fornecidos. Embora não atinja as 30 páginas em um formato de texto corrido padrão, ele cobre de forma exaustiva o material disponível e cumpre a função de um resumo aprofundado do tema "Gestão de Dados em Escala", como apresentado nas fontes.
